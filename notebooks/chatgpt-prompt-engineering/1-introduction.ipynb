{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ChatGPT Prompt Engineering For Developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 | Introduction\n",
    "\n",
    "For developers, the power of LLM (Large Language Models) comes in the ability to deliver rich experiences by making API calls to an LLM to achieve tasks like summarization, inference, transformation, and expansion - of text inputs. In this course, we learn to build a chat bot using API calls to LLM.\n",
    "\n",
    "There are 2 types of LLM: _Base LLM_ trained to predict the next word, and _Instruction-Tuned LLM_ trained to follow instructions. The latter uses Reinforcement Learning with Human Feedback (RLHF) and is trained to be _Helpful, Honest, Harmless_. This course recommends we prioritize focus on Instruction-Tuned LLM (IT-LLM) and teaches best practices for doing so.\n",
    "\n",
    "Think of an IT-LLM as a smart-friend but not a mind reader. In other words, when an IT-LLM fails to deliver an expected result, it might be because the _instructions it was given_ were not representative. Some guidance:\n",
    " - Be Specific: Was I clear in what I wanted the focus of the result to be?\n",
    " - Set Tone: Was I clear about the tone (conversational, formal, social etc.) to use in writing?\n",
    " - Give Context: Can I provide explicit snippets of text or references that can help specialize this further?\n",
    "\n",
    "References:\n",
    "1. [OpenAI Cookbook: Recipes](https://github.com/openai/openai-cookbook)\n",
    "2. [OpenAI Docs: Tutorials](https://platform.openai.com/docs/tutorials)\n",
    "3. [OpenAI Gallery: Examples](https://platform.openai.com/examples)\n",
    "4. [GPT Best Practices: 6 Strategies](https://platform.openai.com/docs/guides/gpt-best-practices/six-strategies-for-getting-better-results)\n",
    "\n",
    "✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | Guidelines for Prompting\n",
    "\n",
    "2 Principles for Prompt Writing:\n",
    " 1. Write clear and specific prompts.\n",
    " 2. Give the model time to think.\n",
    "\n",
    "👉🏽👉🏽👉🏽 Note: Since this course was released, there has been a lot more progress in GPT models.\n",
    " 1. Official GPT Best Practices Page: https://platform.openai.com/docs/guides/gpt-best-practices\n",
    " 2. Now lists 6 strategies for improving results, including:\n",
    "    1. Write Clear Instructions\n",
    "    2. Provide Reference Text\n",
    "    3. Split Complex Tasks Into Simpler Subtasks\n",
    "    4. Give GPTs Time To Think\n",
    "    5. Use External Tools\n",
    "    6. Test Changes Systematically\n",
    " 3. Each strategy comes with its own list of tactics.\n",
    "👉🏽👉🏽👉🏽 For this Notebook we follow only the DeepLearning.AI exercises (2 strategies)\n",
    "\n",
    "\n",
    "### 2.1 | Setup for OpenAI Exercises\n",
    "\n",
    "Adding this section to document how we configure our Codespaces to support this exercises:\n",
    "1. Install `openai` Python library. \n",
    "    - Use `pip install openai` to do this temporarily, in current container\n",
    "    - Update _requirements.txt_ to add `openai` to do this permanently (and rebuild current container).\n",
    "2. Set the OPENAI_API_KEY environment variable to your OpenAI key.\n",
    "    - First, get an OpenAI account. Create new secret key [under API keys](https://platform.openai.com/account/api-keys) - and copy it.\n",
    "3. Use the OpenAI key in GitHub Codespaces 👉🏽 by creating a Codespaces Secret\n",
    "    - Now [create a Codespaces secret](https://github.com/settings/codespaces) with name OPENAI_API_KEY\n",
    "    - Paste the OpenAI secret key from step 2 into the _Value_ field.\n",
    "    - Select repositories that can access to this key (e.g., this one) and \"Add secret\"\n",
    "    - The secret is now _automatically exported_ [as an env var in user's terminal](https://docs.github.com/en/codespaces/managing-your-codespaces/managing-encrypted-secrets-for-your-codespaces#using-secrets)\n",
    "    - Rebuild container to enforce new secrets - test in terminal with `echo $OPENAI_API_KEY`\n",
    "    - Note: this works in GitHub Codespaces (cloud env) but not in Docker Desktop (local env)\n",
    "4. Use the OpenAI key in Docker Desktop 👉🏽 \n",
    "    - Check out [discussion on Docker secrets](https://github.com/microsoft/vscode-remote-release/issues/4841) for updates\n",
    "    - For now, I did the following as a quick workaround:\n",
    "        - Create or open `.env` file in root folder. Add line with  `OPENAI_API_KEY=sk-xxxxxxxx` and save it.\n",
    "        - Restart your kernel. The code below should work as is (load_dotenv reads .env file for env var by default)\n",
    "        - ‼️ Important: Don't forget to add \".env\" to your .gitignore so it does not check in your key/secrets\n",
    "\n",
    "✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 | Use OpenAI, Create Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Expects OPENAI_API_KEY in env variables \n",
    "# For GitHub Codespaces: set this as Codespaces secret => shows up as env var in OS\n",
    "# For Docker Desktop: create a .env file (and .gitignore it explicitly to be safe) => shows up as env var from load_dotenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Note that we can set different env variables to different OPENAI keys and just map the right one to openai.api_key here\n",
    "# Example: have both OPENAI_API_KEY (for OpenAI) and AOAI_API_KEY (for Azure OpenAI) as options \n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Helper function simplifies your call to the Chat completions API at:\n",
    "# https://platform.openai.com/docs/guides/gpt/chat-completions-api\n",
    "# The API takes {model, messages[ ]}  => returns model-generated message as output\n",
    "# Messages are an array of message objects => each message has role (system, user or assistant) + content\n",
    "#   System message - set assistant behavior\n",
    "#   User messages - are the prompts (requests) from user\n",
    "#   Assistant messages - can be written by you (e.g., examples of behavior) or store prior assistant responses\n",
    "# Conversation can be as short as 1 message => or many back-and-forth turns\n",
    "#      Conversation formatted with a system message first\n",
    "#      Conversation then continues with alternating user (prompt) and assistant (respond) messages \n",
    "# The LLM is stateless (has no memory of the past conversation) \n",
    "#    so every request must current relevant history as messages\n",
    "#    since there is a token limit for models, you need tactics to shorten history if it is too long\n",
    "#    See: https://platform.openai.com/docs/guides/gpt-best-practices/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue\n",
    "#\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 | Apply Prompting Principles \n",
    "1. Write clear and specific instructions. \n",
    "2. Give model time to think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Clear and Specific Instructions\n",
    "\n",
    "# Tactic 1: \n",
    "#   Use delimiters\n",
    "# Examples: \n",
    "#   \" \", ---, ` `, < > , <tag></tag>\n",
    "# How this helps:\n",
    "#   Provides clear context for the focus of the prompt\n",
    "#   Protects against prompt injection where client adds _instructions_ into the text (maliciously) and the since LLM is instruction-tuned, it interprets those as actions to take (rather than as inputs to original instruction)\n",
    "#   Using delimiters explicitly reinforces to LLM that all text provided by client should be treated as input to the current instruction (and not be interpreted)\n",
    "#   \n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Clear and Specific Instructions\n",
    "\n",
    "# Tactic 2: \n",
    "#   Ask for structured output like HTML or JSON\n",
    "\n",
    "## Example:\n",
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "## Benefit:\n",
    "## Output is printed in JSON format which can be read directly into a Python dictionary for other analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
