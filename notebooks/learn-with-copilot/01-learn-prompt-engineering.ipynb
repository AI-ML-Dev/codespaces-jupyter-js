{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Introduce Prompt Engineering, its importance in NLP, and how to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Import the required libraries\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Language Models\n",
    "Cover the basics of Language Models, pre-trained models, and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Import the required libraries\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Effective Prompts\n",
    "Discuss the process of designing effective prompts, including choosing the right format, specifying the task and context, and incorporating instructions and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Designing Effective Prompts\n",
    "\n",
    "# Choose the right format\n",
    "prompt_format = \"Write a Python function that takes two numbers as input and returns their sum.\"\n",
    "\n",
    "# Specify the task and context\n",
    "task = \"Addition\"\n",
    "context = \"You are working on a project that requires adding two numbers together. Write a function that can perform this task.\"\n",
    "\n",
    "# Incorporate instructions and constraints\n",
    "instructions = \"Your function should take two arguments, num1 and num2, and return their sum.\"\n",
    "constraints = \"You are not allowed to use the built-in sum() function.\"\n",
    "\n",
    "# Print the prompt details\n",
    "print(\"Prompt Format:\", prompt_format)\n",
    "print(\"Task:\", task)\n",
    "print(\"Context:\", context)\n",
    "print(\"Instructions:\", instructions)\n",
    "print(\"Constraints:\", constraints)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Techniques\n",
    "Explore different techniques for prompt engineering, such as template-based prompts, rule-based prompts, and prompt engineering for text classification and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Template-based prompts\n",
    "template_prompt = f\"{prompt_format}\\n\\n{task}\\n\\n{context}\\n\\n{instructions}\\n\\n{constraints}\"\n",
    "\n",
    "# Rule-based prompts\n",
    "rule_prompt = f\"Task: {task}\\n\\nContext: {context}\\n\\nInstructions: {instructions}\\n\\nConstraints: {constraints}\"\n",
    "\n",
    "# Prompt engineering for text classification and generation\n",
    "classification_prompt = f\"Classification Prompt: {task}\\n\\nContext: {context}\"\n",
    "generation_prompt = f\"Generation Prompt: {task}\\n\\nContext: {context}\"\n",
    "\n",
    "# Print the prompt details\n",
    "print(\"Template-based Prompt:\")\n",
    "print(template_prompt)\n",
    "print(\"\\nRule-based Prompt:\")\n",
    "print(rule_prompt)\n",
    "print(\"\\nClassification Prompt:\")\n",
    "print(classification_prompt)\n",
    "print(\"\\nGeneration Prompt:\")\n",
    "print(generation_prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Iterating Prompts\n",
    "Cover metrics for evaluating prompt performance, analyzing prompt outputs, and iterating and refining prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Import the required libraries\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to evaluate the prompt performance\n",
    "def evaluate_prompt(prompt, model, tokenizer, device):\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the output sequence\n",
    "    output = model.generate(input_ids=input_ids, max_length=100, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the output sequence\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Return the output text\n",
    "    return output_text\n",
    "\n",
    "# Define a function to analyze the prompt outputs\n",
    "def analyze_outputs(outputs):\n",
    "    # Calculate the average length of the outputs\n",
    "    avg_length = np.mean([len(output.split()) for output in outputs])\n",
    "    \n",
    "    # Calculate the diversity of the outputs\n",
    "    unique_outputs = set(outputs)\n",
    "    diversity = len(unique_outputs) / len(outputs)\n",
    "    \n",
    "    # Return the average length and diversity\n",
    "    return avg_length, diversity\n",
    "\n",
    "# Define a function to iterate and refine prompts\n",
    "def iterate_prompts(prompt, model, tokenizer, device, num_iterations):\n",
    "    # Initialize a list to store the refined prompts\n",
    "    refined_prompts = []\n",
    "    \n",
    "    # Iterate for the specified number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Generate a new prompt based on the previous prompt\n",
    "        new_prompt = generate_prompt(prompt, model, tokenizer, device)\n",
    "        \n",
    "        # Add the new prompt to the list of refined prompts\n",
    "        refined_prompts.append(new_prompt)\n",
    "        \n",
    "        # Update the prompt for the next iteration\n",
    "        prompt = new_prompt\n",
    "    \n",
    "    # Return the refined prompts\n",
    "    return refined_prompts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies\n",
    "Examine case studies of applying prompt engineering to specific NLP tasks and successful prompt engineering examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# Case Studies\n",
    "\n",
    "# Case Study 1: Sentiment Analysis\n",
    "\n",
    "# Define the prompt for sentiment analysis\n",
    "sentiment_prompt = \"Classification Prompt: Sentiment Analysis\\n\\nContext: You are working on a project that requires classifying the sentiment of customer reviews. Write a function that can perform this task.\"\n",
    "\n",
    "# Evaluate the prompt performance for sentiment analysis\n",
    "sentiment_output = evaluate_prompt(sentiment_prompt, model, tokenizer, device)\n",
    "\n",
    "# Print the output for sentiment analysis\n",
    "print(\"Sentiment Analysis Output:\")\n",
    "print(sentiment_output)\n",
    "\n",
    "# Case Study 2: Text Generation\n",
    "\n",
    "# Define the prompt for text generation\n",
    "generation_prompt = \"Generation Prompt: Text Generation\\n\\nContext: You are working on a project that requires generating a paragraph of text based on a given topic. Write a function that can perform this task.\"\n",
    "\n",
    "# Evaluate the prompt performance for text generation\n",
    "generation_output = evaluate_prompt(generation_prompt, model, tokenizer, device)\n",
    "\n",
    "# Print the output for text generation\n",
    "print(\"Text Generation Output:\")\n",
    "print(generation_output)\n",
    "\n",
    "# Case Study 3: Question Answering\n",
    "\n",
    "# Define the prompt for question answering\n",
    "qa_prompt = \"Generation Prompt: Question Answering\\n\\nContext: You are working on a project that requires answering questions based on a given passage. Write a function that can perform this task.\"\n",
    "\n",
    "# Evaluate the prompt performance for question answering\n",
    "qa_output = evaluate_prompt(qa_prompt, model, tokenizer, device)\n",
    "\n",
    "# Print the output for question answering\n",
    "print(\"Question Answering Output:\")\n",
    "print(qa_output)\n",
    "\n",
    "# Case Study 4: Named Entity Recognition\n",
    "\n",
    "# Define the prompt for named entity recognition\n",
    "ner_prompt = \"Classification Prompt: Named Entity Recognition\\n\\nContext: You are working on a project that requires identifying and classifying named entities in a given text. Write a function that can perform this task.\"\n",
    "\n",
    "# Evaluate the prompt performance for named entity recognition\n",
    "ner_output = evaluate_prompt(ner_prompt, model, tokenizer, device)\n",
    "\n",
    "# Print the output for named entity recognition\n",
    "print(\"Named Entity Recognition Output:\")\n",
    "print(ner_output)\n",
    "\n",
    "# Case Study 5: Text Summarization\n",
    "\n",
    "# Define the prompt for text summarization\n",
    "summarization_prompt = \"Generation Prompt: Text Summarization\\n\\nContext: You are working on a project that requires generating a summary of a given text. Write a function that can perform this task.\"\n",
    "\n",
    "# Evaluate the prompt performance for text summarization\n",
    "summarization_output = evaluate_prompt(summarization_prompt, model, tokenizer, device)\n",
    "\n",
    "# Print the output for text summarization\n",
    "print(\"Text Summarization Output:\")\n",
    "print(summarization_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Followup Prompt #1\n",
    "\n",
    "`nitya`: What are some best practices for prompt engineering in ChatGPT?\n",
    "\n",
    "`GitHub Copilot`: When it comes to prompt engineering in ChatGPT, here are some best practices to consider:\n",
    "\n",
    "1. **Specify the task and context**: Clearly define the task you want the model to perform and provide relevant context. This helps the model understand the desired outcome and generate accurate responses.\n",
    "\n",
    "2. **Incorporate instructions and constraints**: Include explicit instructions and constraints in the prompt to guide the model's behavior. This helps ensure that the model generates responses that align with your requirements.\n",
    "\n",
    "3. **Choose the right prompt format**: Select a prompt format that suits your task. This could be a question-answer format, a fill-in-the-blank format, or any other format that facilitates effective communication with the model.\n",
    "\n",
    "4. **Start with a template**: Use a template-based approach to structure your prompts. Templates provide a consistent structure and make it easier to generate prompts for different inputs or scenarios.\n",
    "\n",
    "5. **Iterate and refine**: Evaluate the model's responses and iterate on your prompts to improve their effectiveness. Analyze the outputs, identify areas for improvement, and make adjustments to the prompts accordingly.\n",
    "\n",
    "6. **Consider fine-tuning**: If the pre-trained model's performance is not satisfactory, you can consider fine-tuning the model on a specific dataset or task. Fine-tuning can help tailor the model's behavior to better suit your needs.\n",
    "\n",
    "Remember that prompt engineering is an iterative process, and it may require experimentation and fine-tuning to achieve the desired results. Regular evaluation and refinement of prompts are essential for improving the model's performance.\n",
    "\n",
    "Please note that these best practices are general guidelines, and the specific approach may vary depending on the task and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`nitya`: What are some common challenges in prompt engineering for ChatGPT?\n",
    "\n",
    "`GitHub Copilot`: Prompt engineering for ChatGPT can come with its own set of challenges. Here are some common challenges you may encounter:\n",
    "\n",
    "1. **Ambiguity**: Language models like ChatGPT can sometimes interpret prompts ambiguously, leading to unexpected or incorrect responses. It's important to provide clear and unambiguous instructions to minimize this issue.\n",
    "\n",
    "2. **Bias and fairness**: Language models can inadvertently exhibit biases present in the training data. Prompt engineering should consider potential biases and strive for fairness in the generated responses.\n",
    "\n",
    "3. **Over-reliance on prompts**: If the prompts are too specific or rigid, the model may struggle to generalize and provide accurate responses to variations of the same task. Balancing specificity and flexibility is crucial.\n",
    "\n",
    "4. **Lack of control**: ChatGPT models are known to generate creative and sometimes unpredictable responses. Prompt engineering aims to strike a balance between control and creativity, ensuring that the model generates useful and reliable outputs.\n",
    "\n",
    "5. **Evaluation and iteration**: Evaluating the quality of prompt-engineered responses can be challenging. It requires careful analysis, comparison with ground truth, and iterative refinement to improve the model's performance.\n",
    "\n",
    "6. **Domain-specific knowledge**: ChatGPT may lack domain-specific knowledge, and prompt engineering should consider providing necessary context or incorporating external knowledge sources to enhance the model's responses.\n",
    "\n",
    "Addressing these challenges often involves a combination of careful prompt design, continuous evaluation, and fine-tuning of the model. It's important to be aware of these challenges and adapt your prompt engineering approach accordingly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
